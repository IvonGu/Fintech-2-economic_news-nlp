{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#总述\" data-toc-modified-id=\"总述-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>总述</a></span><ul class=\"toc-item\"><li><span><a href=\"#项目简介\" data-toc-modified-id=\"项目简介-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>项目简介</a></span></li><li><span><a href=\"#思路\" data-toc-modified-id=\"思路-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>思路</a></span></li><li><span><a href=\"#各个模型的性能\" data-toc-modified-id=\"各个模型的性能-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>各个模型的性能</a></span></li><li><span><a href=\"#最终代码\" data-toc-modified-id=\"最终代码-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>最终代码</a></span></li></ul></li><li><span><a href=\"#数据预览\" data-toc-modified-id=\"数据预览-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>数据预览</a></span></li><li><span><a href=\"#其他-保存操作\" data-toc-modified-id=\"其他-保存操作-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>其他-保存操作</a></span></li><li><span><a href=\"#简单预处理和特征提取\" data-toc-modified-id=\"简单预处理和特征提取-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>简单预处理和特征提取</a></span><ul class=\"toc-item\"><li><span><a href=\"#预处理\" data-toc-modified-id=\"预处理-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>预处理</a></span><ul class=\"toc-item\"><li><span><a href=\"#分词\" data-toc-modified-id=\"分词-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>分词</a></span></li><li><span><a href=\"#去停用词\" data-toc-modified-id=\"去停用词-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>去停用词</a></span></li><li><span><a href=\"#制作语料库---BOW\" data-toc-modified-id=\"制作语料库---BOW-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>制作语料库 - BOW</a></span></li></ul></li><li><span><a href=\"#特征提取\" data-toc-modified-id=\"特征提取-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>特征提取</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>TF-IDF</a></span></li><li><span><a href=\"#LSI模型---弃\" data-toc-modified-id=\"LSI模型---弃-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>LSI模型 - 弃</a></span></li><li><span><a href=\"#LDA模型---舍弃\" data-toc-modified-id=\"LDA模型---舍弃-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>LDA模型 - 舍弃</a></span></li><li><span><a href=\"#common-words\" data-toc-modified-id=\"common-words-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>common-words</a></span></li><li><span><a href=\"#fuzz-Features\" data-toc-modified-id=\"fuzz-Features-4.2.5\"><span class=\"toc-item-num\">4.2.5&nbsp;&nbsp;</span>fuzz Features</a></span></li><li><span><a href=\"#各种距离---电脑跑不动---耗时弃\" data-toc-modified-id=\"各种距离---电脑跑不动---耗时弃-4.2.6\"><span class=\"toc-item-num\">4.2.6&nbsp;&nbsp;</span>各种距离 - 电脑跑不动 - 耗时弃</a></span><ul class=\"toc-item\"><li><span><a href=\"#词向量相关-wmdistance\" data-toc-modified-id=\"词向量相关-wmdistance-4.2.6.1\"><span class=\"toc-item-num\">4.2.6.1&nbsp;&nbsp;</span>词向量相关-wmdistance</a></span></li><li><span><a href=\"#距离相关的特征，先转成词向量\" data-toc-modified-id=\"距离相关的特征，先转成词向量-4.2.6.2\"><span class=\"toc-item-num\">4.2.6.2&nbsp;&nbsp;</span>距离相关的特征，先转成词向量</a></span></li></ul></li><li><span><a href=\"#BM25-√\" data-toc-modified-id=\"BM25-√-4.2.7\"><span class=\"toc-item-num\">4.2.7&nbsp;&nbsp;</span>BM25 √</a></span></li></ul></li></ul></li><li><span><a href=\"#结果提交\" data-toc-modified-id=\"结果提交-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>结果提交</a></span></li><li><span><a href=\"#总结分析\" data-toc-modified-id=\"总结分析-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>总结分析</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目简介\n",
    "\n",
    "> 无监督-财经新闻相似度分析\n",
    "\n",
    "> 针对test_data中的新闻, 分别在train_data中找到最相似的TOP20个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 思路\n",
    "\n",
    "> 第一次做无监督,首先利用常见的模型-similarity排序-定为baseline\n",
    "\n",
    "> 后来看到大神提到的BM25-baseline方法\n",
    "\n",
    "<font color = 'blue'>\n",
    "\n",
    "**总结**\n",
    "\n",
    "> **虽然提了很多个特征来求相似度，但是组合起来提交后结果并不好。**\n",
    "\n",
    "> **最终采取BM25调参-结果0.1185**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各个模型的性能\n",
    "\n",
    "> 这里预处理只是简单的分词\n",
    "\n",
    "> lda特别不靠谱，只有0.00几-舍弃\n",
    "\n",
    "* lsi和lda可能要去停用词？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>\n",
    "\n",
    "> **各个模型性能如下表所示**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | 模型 | 性能 | 注 |\n",
    "| ------ | ------ | ------ | ------ |\n",
    "| genism-API-bm25-1 | **bm25** | **0.114** | 未调参 |\n",
    "| genism-API-bm25-2 | bm25-人工处理test | 0.092 | 人为去掉某些文字，导致性能恶化 |\n",
    "| 较常用模型1 | TFIDF | 0.0908 |  |\n",
    "| 较常用模型2 | lsi-50 | 0.0696 | 性能会随着主题数增大,但是渐渐性能提升较小 |\n",
    "| 字面特征 | common_words | 0.075 |  |\n",
    "| fuzz特征1 | Qratio | 0.0784 |  |\n",
    "| fuzz特征2 | Wratio | 0.0379 |  |\n",
    "| fuzz特征3 | partial ratio | 0.0569 | 可能只包含短语 |\n",
    "| fuzz特征4 | partial_token_set_ratio | 0.057 |  |\n",
    "| fuzz特征5 | partial_token_sort_ratio | 0.025 |  |\n",
    "| fuzz特征6 | token_set_ratio | 0.0668 |  |\n",
    "| fuzz特征7 | token_sort_ratio | 0.0700 |  |\n",
    "| 组合特征1 | Qratio+ Wratio | 0.0717 |  |\n",
    "| 组合特征2 | tf + lsi | 0.0898 |  |\n",
    "| 组合特征3 | tf + comwords | 0.0929 |  |\n",
    "| 组合特征4 | tf + Qratio | 0.0937 |  |\n",
    "| 组合特征5 | tf + Qratio + comwords | 0.0896 |  |\n",
    "| 组合特征6 | bm25+TFIDF | 0.1045 |  |\n",
    "| 组合特征7 | bm25_100+TFIDF | 0.1042 | bm25直接除以100 |\n",
    "| 组合特征8 | bm25_100+TFIDF_0.7 | 0.1052 | bm25直接除以100,TFIDF权重0.7 |\n",
    "| 组合特征9 | all1 | 0.0908 | bm归一化0-1 |\n",
    "| 组合特征10 | all2 | 0.1127 | bm没归一化 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最终代码\n",
    "\n",
    "> BM25调参-b=0.85,k=1.4--0.1185\n",
    "\n",
    "> 应该还可以继续调参，因为时间原因——未调到最优值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.读取和分词**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "df_train = pd.read_csv('train_data.csv')\n",
    "df_test = pd.read_csv('test_data.csv', encoding = 'gbk')\n",
    "\n",
    "# 分词\n",
    "all_doc_list = []\n",
    "for doc in df_train.title:\n",
    "    doc_list = [word for word in jieba.cut(doc)]\n",
    "    all_doc_list.append(doc_list)\n",
    "\n",
    "test_doc_list = []\n",
    "for doc in df_test.title:\n",
    "    doc_list = [word for word in jieba.cut(doc)]\n",
    "    test_doc_list.append(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.下面是BM25算法-直接复制的电脑gensim文件夹下的源码**\n",
    "\n",
    "> K1官方范围是1.2—2.0,越小饱和过程越快; b范围是0—1,表示归约化程度。\n",
    "\n",
    "> 台式机是gensim1.0效果很差，笔记本gensim3.0结果正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BM25\n",
    "import math\n",
    "from six import iteritems\n",
    "from six.moves import xrange\n",
    "PARAM_K1 = 1.4\n",
    "PARAM_B = 0.85\n",
    "EPSILON = 0.25\n",
    "\n",
    "\n",
    "class BM25(object):\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : list of list of str\n",
    "            Given corpus.\n",
    "\n",
    "        \"\"\"\n",
    "        self.corpus_size = len(corpus)\n",
    "        self.avgdl = sum(float(len(x)) for x in corpus) / self.corpus_size\n",
    "        self.corpus = corpus\n",
    "        self.f = []\n",
    "        self.df = {}\n",
    "        self.idf = {}\n",
    "        self.doc_len = []\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Calculates frequencies of terms in documents and in corpus. Also computes inverse document frequencies.\"\"\"\n",
    "        for document in self.corpus:\n",
    "            frequencies = {}\n",
    "            self.doc_len.append(len(document))\n",
    "            for word in document:\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 0\n",
    "                frequencies[word] += 1\n",
    "            self.f.append(frequencies)\n",
    "\n",
    "            for word, freq in iteritems(frequencies):\n",
    "                if word not in self.df:\n",
    "                    self.df[word] = 0\n",
    "                self.df[word] += 1\n",
    "\n",
    "        for word, freq in iteritems(self.df):\n",
    "            self.idf[word] = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
    "\n",
    "    def get_score(self, document, index, average_idf):\n",
    "        score = 0\n",
    "        for word in document:\n",
    "            if word not in self.f[index]:\n",
    "                continue\n",
    "            idf = self.idf[word] if self.idf[word] >= 0 else EPSILON * average_idf\n",
    "            score += (idf * self.f[index][word] * (PARAM_K1 + 1)\n",
    "                      / (self.f[index][word] + PARAM_K1 * (1 - PARAM_B + PARAM_B * self.doc_len[index] / self.avgdl)))\n",
    "        return score\n",
    "\n",
    "    def get_scores(self, document, average_idf):\n",
    "        scores = []\n",
    "        for index in xrange(self.corpus_size):\n",
    "            score = self.get_score(document, index, average_idf)\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "\n",
    "\n",
    "# def get_bm25_weights(corpus):\n",
    "#     average_idf = sum(float(val) for val in bm25.idf.values()) / len(bm25.idf)\n",
    "\n",
    "#     weights = []\n",
    "#     for doc in corpus:\n",
    "#         scores = bm25.get_scores(doc, average_idf)\n",
    "#         weights.append(scores)\n",
    "\n",
    "#     return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.运用BM25**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 用gensim建立BM25模型\n",
    "bm25Model = BM25(all_doc_list)\n",
    "# 根据gensim源码，计算平均逆文档频率\n",
    "average_idf = sum(float(val) for val in bm25Model.idf.values()) / len(bm25Model.idf)\n",
    "\n",
    "# 对测试集中每一个样本-BM25\n",
    "sim_bm25 = []\n",
    "for query in test_doc_list:\n",
    "    sim_bm25.append(bm25Model.get_scores(query,average_idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.代码提交**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# id-title映射,方便在文件提交时显示title-人工判别结果是否合理\n",
    "dic_id_title = {}\n",
    "for i in range(len(df_train.id)):\n",
    "    dic_id_title[df_train.id[i]] = df_train.title[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim = sim_bm25\n",
    "\n",
    "# 保存TOP21的id和相似度\n",
    "res = []\n",
    "prob = []\n",
    "for x in sim:\n",
    "    similar_sorted = sorted(enumerate(x), key=lambda item: -item[1])[:21]    # 升序排序 \n",
    "    idxs = [str(item[0] + 1) for item in similar_sorted]\n",
    "    simm = [str(item[1]) for item in similar_sorted]\n",
    "    res.append(idxs)\n",
    "    prob.append(simm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**首先在TOP21中寻找source-id**\n",
    "\n",
    "> 找到的话索引记为idx,将0—idx-1和idx—21作为最终的TOP20结果\n",
    "\n",
    "> 找不到的话(except)吧TOP20作为最终结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_id = []\n",
    "target_id = []\n",
    "similarity = []\n",
    "source_title = []\n",
    "target_title = []\n",
    "for i in range(len(res)):\n",
    "    item = res[i]\n",
    "    try:\n",
    "        idx = item.index(str(list(df_test.id)[i])) # 找到索引\n",
    "\n",
    "        source_id.extend([item[idx]] * 20)\n",
    "        target_id.extend(item[:idx] + item[idx+1:])\n",
    "\n",
    "        similarity.extend(prob[i][:idx] + prob[i][idx+1:])\n",
    "        \n",
    "        source_title.extend([dic_id_title[int(item[idx])]] * 20)\n",
    "        \n",
    "        target_title.extend([dic_id_title[int(x)] for x in target_id[-20:]])\n",
    "    except:    # 没找到索引\n",
    "        source = str(list(df_test.id)[i])\n",
    "        \n",
    "        source_id.extend([source] * 20)\n",
    "        target_id.extend(item[:20])\n",
    "\n",
    "        similarity.extend(prob[i][:20])\n",
    "        \n",
    "        source_title.extend([dic_id_title[int(source)]] * 20)       \n",
    "        target_title.extend([dic_id_title[int(x)] for x in target_id[-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['source_id'] = source_id\n",
    "submission['target_id'] = target_id\n",
    "submission['similarity'] = similarity\n",
    "\n",
    "submission['source_title'] = source_title\n",
    "submission['target_title'] = target_title\n",
    "\n",
    "submission.to_csv('sub_bm25_K_2.txt', index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                              title\n",
      "0   1                       大众交通10月10日至12日减持国泰君安股票705万股。\n",
      "1   2                        欧洲地中海地震中心： 阿留申群岛区域发生5.0级地震。\n",
      "2   3  【马尔代夫政局再添不稳 司法部长称最高法要弹劾总统】据路透社报道，当地时间4日，马尔代夫司法...\n",
      "3   4  【**绿地控股拟定增募资逾300亿元布局多元化产业**】绿地控股发布定增预案，公司拟以不低于...\n",
      "4   5                  英国退欧大臣戴维斯：就北爱尔兰达成的退欧后监管将适用于英国其余地区\n",
      "5   6                                南玻A直线拉升，从跌超1%到涨逾2%。\n",
      "6   7              \"美国芝加哥联储主席Evans（2017年有投票权）：加息的上方空间减少。\n",
      "7   8  整点回顾：市场分化明显，沪指受到券商板块的护盘小幅下挫，而中小创受到次新股的拖累，跌逾1%，...\n",
      "8   9                       【提示】LME铅库存增加0.8%，为9月初以来最大涨幅。\n",
      "9  10                           日本6月制造业PMI终值50.1，初值49.9。\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "      id                                        title\n",
      "0   2784               美国地质勘探局：印度莫黑安东南部151公里处发生5.1级地震\n",
      "1  12057                       美国MBA抵押贷款申请指数上周上升2.7%。\n",
      "2  19620                  美国德州市长：预计将向印度供应更多的原油和液化天然气。\n",
      "3  23768                         欧洲央行月报：仍需要足够程度的货币宽松。\n",
      "4  23814      IMF：2017年11月委内瑞拉黄金持有量下降6.373吨至184.939吨。\n",
      "5  28091                        YTN：朝鲜金正恩称，他将会和美国举行峰会\n",
      "6  34581  美国副总统彭斯：（就朝鲜问题）为所有可能的结果做好准备将对任何核武器的使用进行快速应对\n",
      "7  46930         香港财政司长陈茂波：需要继续留意全球货币环境和地缘政治的变化与政策风险。\n",
      "8  51579               日本央行理事雨宫正佳：退出宽松的细节将由经济和物价状况决定。\n",
      "9  62560           日本央行新任副行长：日本不存在通缩，但是距离2%的通胀目标仍有距离。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('train_data.csv')\n",
    "df_test = pd.read_csv('test_data.csv', encoding = 'gbk')\n",
    "print(df_train.head(10))\n",
    "print('- -'*25)\n",
    "print(df_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(485686, 2)\n",
      "(50, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他-保存操作\n",
    "\n",
    "> 跑各个特征太慢了,这里跑完后保存下。\n",
    "\n",
    ">  这里用pickle保存之前运行过的变量，下次直接load。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dump保存过程**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data1.pickle','wb') as f:\n",
    "    pickle.dump([corpus,corpus_tfidf,sim_tfidf,sim_lsi,common_words],f)\n",
    "\n",
    "# fuzz特征\n",
    "with open('fuzz_Qratio.pickle','wb') as f:\n",
    "    pickle.dump(fuzz_Qratio,f)\n",
    "with open('fuzz_Wratio.pickle','wb') as f:\n",
    "    pickle.dump(fuzz_Wratio,f)\n",
    "with open('fuzz_partial_ratio.pickle','wb') as f:\n",
    "    pickle.dump(fuzz_partial_ratio,f)\n",
    "with open('fuzz_partial_token_set_ratio.pickle','wb') as f:\n",
    "    pickle.dump(fuzz_partial_token_set_ratio,f)\n",
    "with open('fuzz_partial_token_sort_ratio.pickle','wb') as f:\n",
    "    pickle.dump(fuzz_partial_token_sort_ratio,f)\n",
    "with open('fuzz_token_set_ratio.pickle','wb') as f:\n",
    "    pickle.dump(fuzz_token_set_ratio,f)\n",
    "with open('fuzz_token_sort_ratio.pickle','wb') as f:\n",
    "    pickle.dump(fuzz_token_sort_ratio,f)\n",
    "    \n",
    "# 距离特征norm_wmdis\n",
    "with open('wmdis.pickle','wb') as f:\n",
    "    pickle.dump(wmdis,f)\n",
    "with open('norm_wmdis.pickle','wb') as f:\n",
    "    pickle.dump(norm_wmdis,f)\n",
    "    \n",
    "with open('cosine.pickle', 'wb') as f:\n",
    "    pickle.dump(cosine, f)\n",
    "with open('manhattan.pickle', 'wb') as f:\n",
    "    pickle.dump(manhattan, f)\n",
    "with open('canberra.pickle', 'wb') as f:\n",
    "    pickle.dump(canberra, f)\n",
    "with open('euclidean.pickle', 'wb') as f:\n",
    "    pickle.dump(euclidean, f)\n",
    "with open('minkowski.pickle', 'wb') as f:\n",
    "    pickle.dump(minkowski, f)\n",
    "with open('braycurtis.pickle', 'wb') as f:\n",
    "    pickle.dump(braycurtis, f) \n",
    "    \n",
    "# id-title的映射\n",
    "with open('dic_id_title.pickle','wb') as f:\n",
    "    pickle.load(dic_id_title, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load载入过程**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# id-title的映射\n",
    "with open('dic_id_title.pickle','rb') as f:\n",
    "    dic_id_title = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data1.pickle','rb') as f:\n",
    "    [corpus,corpus_tfidf,sim_tfidf,sim_lsi,common_words] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('fuzz_Qratio.pickle','rb') as f:\n",
    "    fuzz_Qratio = pickle.load(f)\n",
    "with open('fuzz_token_set_ratio.pickle','rb') as f:\n",
    "    fuzz_token_set_ratio = pickle.load(f)\n",
    "with open('fuzz_token_sort_ratio.pickle','rb') as f:\n",
    "    fuzz_token_sort_ratio = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单预处理和特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单的文本相似度分析**\n",
    "\n",
    "> https://blog.csdn.net/xiexf189/article/details/79092629"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理\n",
    "\n",
    "最终决定--只分词，不去停用词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词\n",
    "\n",
    "> jieba.cut(句子)来分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\XPS13\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.817 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "all_doc_list = []\n",
    "for doc in df_train.title:\n",
    "    doc_list = [word for word in jieba.cut(doc)]\n",
    "    all_doc_list.append(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_doc_list = []\n",
    "for doc in df_test.title:\n",
    "    doc_list = [word for word in jieba.cut(doc)]\n",
    "    test_doc_list.append(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>\n",
    "\n",
    "#### 去停用词\n",
    "\n",
    "<font color = 'black'>\n",
    "\n",
    "> 不去了，只针对TFIDF模型结果会变差。\n",
    "\n",
    "> **猜测原因：**可能会让TOP20没有区分度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = ['【','】','(',')','\"','*']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 制作语料库 - BOW\n",
    "\n",
    "> 语料库是一组向量，向量中的元素是一个**二元组（编号、频次数）**，对应分词后的文档中的每一个词。\n",
    "\n",
    "> corpora.Dictionary(词语列表) # 用dictionary方法获取词袋（bag-of-words)\n",
    "\n",
    "> dictionary.keys()表示每个词的编号, dictionary.token2id表示编号和词的对应关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(all_doc_list)    # 用dictionary方法获取词袋（bag-of-words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258747"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary.keys()) #词袋中用数字对所有词进行了编号\n",
    "dictionary.token2id# 编号与词之间的对应关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 用doc2bow制作语料库\n",
    "corpus = [dictionary.doc2bow(doc) for doc in all_doc_list]\n",
    "doc_test_vec= [dictionary.doc2bow(doc) for doc in test_doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)], [(3, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)]]\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "[[(15, 1), (17, 1), (19, 1), (22, 1), (162, 1), (1903, 1), (2651, 1), (4173, 1), (10674, 1), (15563, 1), (15629, 1), (15630, 1), (15631, 1), (15632, 1)], [(3, 1), (162, 1), (191, 1), (357, 1), (358, 1), (359, 1), (360, 1), (361, 1), (362, 1), (4635, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:2])\n",
    "print('- -'*25)\n",
    "print(doc_test_vec[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF \n",
    "\n",
    "> 用TF-IDF模型对语料库（包含了训练集和测试集）建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)    # 建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 对每个目标文档，分析测试文档的相似度\n",
    "\n",
    "tfidf[corpus],tfidf[doc_test_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "index = similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=len(dictionary.keys()))\n",
    "sim_tfidf = index[tfidf[doc_test_vec]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 485686)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSI模型 - 弃\n",
    "\n",
    "> 训练文档向量组成的矩阵SVD分解，并做了一个秩为2的近似SVD分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.247*\"、\" + 0.215*\"，\" + 0.186*\"的\" + 0.167*\"月\" + 0.154*\"*\" + 0.130*\" \" + 0.126*\"将\" + 0.123*\"美元\" + 0.119*\"年\" + 0.118*\"\"\"'),\n",
       " (1,\n",
       "  '-0.274*\"点\" + -0.265*\"美元\" + 0.240*\"*\" + -0.221*\"报\" + -0.193*\"指数\" + -0.186*\"/\" + 0.160*\"的\" + -0.156*\"至\" + -0.146*\"跌幅\" + -0.142*\"跌\"'),\n",
       " (2,\n",
       "  '0.485*\"*\" + 0.478*\"、\" + 0.238*\"沪\" + 0.188*\"涨\" + 0.129*\"板块\" + -0.124*\"美国\" + -0.120*\"月\" + 0.118*\"跌\" + -0.109*\"美元\" + -0.103*\"/\"'),\n",
       " (3,\n",
       "  '-0.800*\"*\" + 0.389*\"、\" + 0.123*\"沪\" + -0.123*\"\"\" + 0.105*\"涨\" + -0.095*\"美元\" + 0.082*\"板块\" + -0.079*\"/\" + 0.078*\"等\" + -0.064*\"月\"'),\n",
       " (4,\n",
       "  '0.323*\"前值\" + 0.273*\"-\" + 0.240*\"预期\" + -0.227*\"美元\" + 0.220*\"月\" + 0.194*\" \" + 0.176*\"环比\" + -0.168*\"点\" + 0.161*\"同比\" + 0.135*\"亿元\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 有了这个lsi模型，我们就可以将文档映射到一个二维的topic空间中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算相似度\n",
    "corpus_lsi = lsi[corpus]  # for doc in corpus_lsi\n",
    "index = similarities.MatrixSimilarity(corpus_lsi)\n",
    "sim_lsi = index[lsi[doc_test_vec]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 485686)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_lsi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA模型 - 舍弃\n",
    "\n",
    "> lda模型中的每个主题单词都有概率意义，其加和为1，值越大权重越大，物理意义比较明确，不过反过来再看这三篇文档训练的2个主题的LDA模型太平均了，没有说服力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20,\n",
       "  '0.015*\"视网\" + 0.014*\"乐\" + 0.012*\"午盘\" + 0.011*\"沙特\" + 0.011*\"额度\" + 0.010*\"亿元\" + 0.010*\"股通\" + 0.009*\"剩余\" + 0.009*\"占\" + 0.009*\"130\"'),\n",
       " (5,\n",
       "  '0.032*\"人民币\" + 0.024*\"兑\" + 0.016*\"美元\" + 0.015*\"在岸\" + 0.014*\"中间价\" + 0.013*\"报\" + 0.013*\"亿元\" + 0.011*\"余额\" + 0.011*\"融资\" + 0.011*\"离岸\"'),\n",
       " (11,\n",
       "  '0.008*\"流出\" + 0.008*\"两年\" + 0.007*\"资金\" + 0.007*\"净流入\" + 0.007*\"成交\" + 0.007*\"基金\" + 0.006*\"再次\" + 0.006*\"员工\" + 0.006*\"双双\" + 0.006*\"零\"'),\n",
       " (15,\n",
       "  '0.014*\"、\" + 0.009*\"发展\" + 0.009*\"建设\" + 0.008*\"工作\" + 0.008*\"推进\" + 0.007*\"改革\" + 0.007*\".\" + 0.007*\"合作\" + 0.007*\"企业\" + 0.006*\"》\"'),\n",
       " (10,\n",
       "  '0.016*\"汽车\" + 0.013*\"新能源\" + 0.011*\"直线\" + 0.009*\"封板\" + 0.009*\"8%\" + 0.009*\"恒生\" + 0.008*\"龙头\" + 0.008*\"顾问\" + 0.007*\"市值\" + 0.007*\"感到\"')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算相似度\n",
    "corpus_lda = lda[corpus]\n",
    "index = similarities.MatrixSimilarity(corpus_lda)\n",
    "sim_lda = index[lda[doc_test_vec]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_lda.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### common-words\n",
    "\n",
    "> 注意下面把权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 共同单词个数 / 最长总单词\n",
    "common_words = []\n",
    "for i in df_test.title:\n",
    "    temp = []\n",
    "    for j in df_train.title:\n",
    "        temp.append(len(set(i).intersection(set(j))) / max(len(set(i)),len(set(j))))\n",
    "    common_words.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 可能需要依靠前100个？\n",
    "# for row in common_words:\n",
    "#     similar_sorted = sorted(enumerate(row), key=lambda item: -item[1])[:100]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fuzz Features\n",
    "\n",
    "> fuzz_Qratio,fuzz_Wratio,fuzz_partial_ratio,fuzz_partial_token_set_ratio,\n",
    "\n",
    "> fuzz_partial_token_sort_ratio,fuzz_token_set_ratio,fuzz_token_sort_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fuzz_Qratio = []\n",
    "for i in df_test.title:\n",
    "    temp = [fuzz.QRatio(i,j) for j in df_train.title]\n",
    "    fuzz_Qratio.append(temp)\n",
    "    \n",
    "fuzz_Wratio = []\n",
    "for i in df_test.title:\n",
    "    temp = [fuzz.WRatio(i,j) for j in df_train.title]\n",
    "    fuzz_Wratio.append(temp)\n",
    "    \n",
    "fuzz_partial_ratio = []\n",
    "for i in df_test.title:\n",
    "    temp = [fuzz.partial_ratio(i,j) for j in df_train.title]\n",
    "    fuzz_partial_ratio.append(temp)\n",
    "    \n",
    "fuzz_partial_token_set_ratio = []\n",
    "for i in df_test.title:\n",
    "    temp = [fuzz.partial_token_set_ratio(i,j) for j in df_train.title]\n",
    "    fuzz_partial_token_set_ratio.append(temp)\n",
    "    \n",
    "fuzz_partial_token_sort_ratio = []\n",
    "for i in df_test.title:\n",
    "    temp = [fuzz.partial_token_sort_ratio(i,j) for j in df_train.title]\n",
    "    fuzz_partial_token_sort_ratio.append(temp)\n",
    "    \n",
    "fuzz_token_set_ratio = []\n",
    "for i in df_test.title:\n",
    "    temp = [fuzz.token_set_ratio(i,j) for j in df_train.title]\n",
    "    fuzz_token_set_ratio.append(temp)\n",
    "\n",
    "fuzz_token_sort_ratio = []\n",
    "for i in df_test.title:\n",
    "    temp = [fuzz.token_sort_ratio(i,j) for j in df_train.title]\n",
    "    fuzz_token_sort_ratio.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 各种距离 - 电脑跑不动 - 耗时弃\n",
    "\n",
    "> 越小越近\n",
    "\n",
    "> **计算机跑的太慢，放弃**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 词向量相关-wmdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "# 特征1：wmdis\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('word2vec/hanlp-wiki-vec-zh.txt')\n",
    "wmdis = []\n",
    "for i in test_doc_list:\n",
    "    print('----'+idx+'------')\n",
    "    idx += 1\n",
    "    temp = [model.wmdistance(i, j) for j in all_doc_list]\n",
    "    wmdis.append(temp)\n",
    "    \n",
    "with open('wmdis.pickle', 'wb') as f:\n",
    "    pickle.dump(wmdis, f)\n",
    "    \n",
    "# 特征2：norm_wmdis\n",
    "norm_model = gensim.models.KeyedVectors.load_word2vec_format('word2vec/hanlp-wiki-vec-zh.txt')\n",
    "norm_model.init_sims(replace=True)\n",
    "norm_wmdis = []\n",
    "for i in test_doc_list:\n",
    "    temp = [norm_model.wmdistance(i, j) for j in all_doc_list]\n",
    "    norm_wmdis.append(temp)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 距离相关的特征，先转成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2vec(words):\n",
    "    m = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            m.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    m = np.array(m)\n",
    "    v = m.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_vec = [sent2vec(i) for i in test_doc_list]\n",
    "df_test_vec = np.array(df_test_vec)\n",
    "df_train_vec = [sent2vec(i) for i in all_doc_list]\n",
    "df_train_vec = np.array(df_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 各种距离\n",
    "cosine = []\n",
    "for i in df_test_vec:\n",
    "    temp = [distance.cosine(i, j) for j in df_train_vec]\n",
    "    cosine.append(temp)\n",
    " \n",
    "manhattan = []\n",
    "for i in df_test_vec:\n",
    "    temp = [distance.cityblock(i, j) for j in df_train_vec]\n",
    "    manhattan.append(temp)\n",
    "\n",
    "canberra = []\n",
    "for i in df_test_vec:\n",
    "    temp = [distance.canberra(i, j) for j in df_train_vec]\n",
    "    canberra.append(temp)\n",
    "\n",
    "\n",
    "euclidean = []\n",
    "for i in df_test_vec:\n",
    "    temp = [distance.euclidean(i, j) for j in df_train_vec]\n",
    "    euclidean.append(temp)\n",
    "\n",
    "minkowski = []\n",
    "for i in df_test_vec:\n",
    "    temp = [distance.minkowski(i, j) for j in df_train_vec]\n",
    "    minkowski.append(temp)\n",
    "\n",
    "braycurtis = []\n",
    "for i in df_test_vec:\n",
    "    temp = [distance.braycurtis(i, j) for j in df_train_vec]\n",
    "    braycurtis.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### BM25 √"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from six import iteritems\n",
    "from six.moves import xrange\n",
    "\n",
    "\n",
    "PARAM_K1 = 1.4\n",
    "PARAM_B = 0.85\n",
    "EPSILON = 0.25\n",
    "\n",
    "\n",
    "class BM25(object):\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : list of list of str\n",
    "            Given corpus.\n",
    "\n",
    "        \"\"\"\n",
    "        self.corpus_size = len(corpus)\n",
    "        self.avgdl = sum(float(len(x)) for x in corpus) / self.corpus_size\n",
    "        self.corpus = corpus\n",
    "        self.f = []\n",
    "        self.df = {}\n",
    "        self.idf = {}\n",
    "        self.doc_len = []\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Calculates frequencies of terms in documents and in corpus. Also computes inverse document frequencies.\"\"\"\n",
    "        for document in self.corpus:\n",
    "            frequencies = {}\n",
    "            self.doc_len.append(len(document))\n",
    "            for word in document:\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 0\n",
    "                frequencies[word] += 1\n",
    "            self.f.append(frequencies)\n",
    "\n",
    "            for word, freq in iteritems(frequencies):\n",
    "                if word not in self.df:\n",
    "                    self.df[word] = 0\n",
    "                self.df[word] += 1\n",
    "\n",
    "        for word, freq in iteritems(self.df):\n",
    "            self.idf[word] = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
    "\n",
    "    def get_score(self, document, index, average_idf):\n",
    "        score = 0\n",
    "        for word in document:\n",
    "            if word not in self.f[index]:\n",
    "                continue\n",
    "            idf = self.idf[word] if self.idf[word] >= 0 else EPSILON * average_idf\n",
    "            score += (idf * self.f[index][word] * (PARAM_K1 + 1)\n",
    "                      / (self.f[index][word] + PARAM_K1 * (1 - PARAM_B + PARAM_B * self.doc_len[index] / self.avgdl)))\n",
    "        return score\n",
    "\n",
    "    def get_scores(self, document, average_idf):\n",
    "        scores = []\n",
    "        for index in xrange(self.corpus_size):\n",
    "            score = self.get_score(document, index, average_idf)\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "\n",
    "\n",
    "def get_bm25_weights(corpus):\n",
    "    average_idf = sum(float(val) for val in bm25.idf.values()) / len(bm25.idf)\n",
    "\n",
    "    weights = []\n",
    "    for doc in corpus:\n",
    "        scores = bm25.get_scores(doc, average_idf)\n",
    "        weights.append(scores)\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from gensim.summarization import bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 用gensim建立BM25模型\n",
    "bm25Model = BM25(all_doc_list)\n",
    "# 根据gensim源码，计算平均逆文档频率\n",
    "average_idf = sum(float(val) for val in bm25Model.idf.values()) / len(bm25Model.idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_bm25 = []\n",
    "for query in test_doc_list:\n",
    "    sim_bm25.append(bm25Model.get_scores(query,average_idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果提交\n",
    "\n",
    "> 归一化softmax不合适"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**建立id-title映射，方便在文件提交时显示title-人工判别结果是否合理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic_id_title = {}\n",
    "for i in range(len(df_train.id)):\n",
    "    dic_id_title[df_train.id[i]] = df_train.title[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def max_normal(x):\n",
    "#     min_x = min(x)\n",
    "#     max_x = max(x)\n",
    "#     return [(i - min_x)/(max_x - min_x) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sim1_bm25 = []\n",
    "# for x in sim_bm25:\n",
    "#     sim1_bm25.append(max_normal(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sim_bm251 = np.array(sim_bm25)\n",
    "# common_words1 = np.array(common_words)\n",
    "# sim_lsi1 = np.array(sim_lsi)\n",
    "# fuzz_Qratio1 = np.array(fuzz_Qratio)/100\n",
    "# fuzz_token_set_ratio1 = np.array(fuzz_token_set_ratio)/100\n",
    "# fuzz_token_sort_ratio1 = np.array(fuzz_token_sort_ratio)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sim = sim_bm25 + common_words1 * 0.6 + fuzz_Qratio1 * 0.6 + \\\n",
    "# fuzz_token_set_ratio1 * 0.2 + fuzz_token_sort_ratio1 * 0.3 +sim_lsi1 * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sim = fuzz_token_set_ratio\n",
    "# # sim = sim_tfidf + sim1_bm25\n",
    "# sim = sim_bm251 + sim_tfidf * 0.7\n",
    "\n",
    "# # fuzz_Qratio的取值在0-100之间\n",
    "# fuzz_Qratio1 = np.array(fuzz_Qratio)/100\n",
    "\n",
    "# fuzz_Wratio1 = np.array(fuzz_Wratio)/100\n",
    "# sim = fuzz_Wratio1\n",
    "\n",
    "# fuzz_partial_ratio1 = np.array(fuzz_partial_ratio)/100\n",
    "# sim = fuzz_partial_ratio1\n",
    "\n",
    "# # sim = sim_tfidf + fuzz_Qratio1 + common_words\n",
    "# #sim = (sim_tfidf + sim_lsi) / 2\n",
    "# # sim = sim_lda\n",
    "# # sim = common_words + sim_tfidf\n",
    "# # sim = fuzz_Qratio\n",
    "# sim = fuzz_Wratio1 + fuzz_Qratio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim = sim_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存TOP21的id和相似度\n",
    "res = []\n",
    "prob = []\n",
    "for x in sim:\n",
    "    similar_sorted = sorted(enumerate(x), key=lambda item: -item[1])[:21]    # 升序排序 \n",
    "    idxs = [str(item[0] + 1) for item in similar_sorted]\n",
    "    simm = [str(item[1]) for item in similar_sorted]\n",
    "    res.append(idxs)\n",
    "    prob.append(simm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**首先在TOP21中寻找source-id**\n",
    "\n",
    "> 找到的话索引记为idx,将0—idx-1和idx—21作为最终的TOP20结果\n",
    "\n",
    "> 找不到的话(except)吧TOP20作为最终结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_id = []\n",
    "target_id = []\n",
    "similarity = []\n",
    "source_title = []\n",
    "target_title = []\n",
    "for i in range(len(res)):\n",
    "    item = res[i]\n",
    "    try:\n",
    "        idx = item.index(str(list(df_test.id)[i])) # 找到索引\n",
    "\n",
    "        source_id.extend([item[idx]] * 20)\n",
    "        target_id.extend(item[:idx] + item[idx+1:])\n",
    "\n",
    "        similarity.extend(prob[i][:idx] + prob[i][idx+1:])\n",
    "        \n",
    "        source_title.extend([dic_id_title[int(item[idx])]] * 20)\n",
    "        \n",
    "        target_title.extend([dic_id_title[int(x)] for x in target_id[-20:]])\n",
    "    except:    # 没找到索引\n",
    "        source = str(list(df_test.id)[i])\n",
    "        \n",
    "        source_id.extend([source] * 20)\n",
    "        target_id.extend(item[:20])\n",
    "\n",
    "        similarity.extend(prob[i][:20])\n",
    "        \n",
    "        source_title.extend([dic_id_title[int(source)]] * 20)       \n",
    "        target_title.extend([dic_id_title[int(x)] for x in target_id[-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['source_id'] = source_id\n",
    "submission['target_id'] = target_id\n",
    "submission['similarity'] = similarity\n",
    "\n",
    "submission['source_title'] = source_title\n",
    "submission['target_title'] = target_title\n",
    "\n",
    "submission.to_csv('sub_bm25_K_2.txt', index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结分析\n",
    "\n",
    "> 选用的模型很重要..第一次听说BM25\n",
    "\n",
    "> 多看论文,复现方法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
